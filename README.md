# Text-Summarization: A Production-Grade NLP Summarization System

An end-to-end, NLP-driven pipeline for automated text summarization engineered for modern content processing needs. By leveraging state-of-the-art Hugging Face Transformers, PyTorch acceleration, and modular MLOps practices, this system delivers accurate, concise summariesâ€”ready for production deployment at scale.

---

## ğŸ“‚ Repository Structure
```
.
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/             # CI/CD pipeline workflows for automated deployment
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml            # Project configuration: artifact paths, model settings, data sources
â”œâ”€â”€ less_records_artifacts/    # Sample artifacts from experiments with smaller dataset
â”œâ”€â”€ notebook/                  # Jupyter notebooks for experimentation and prototyping
â”‚   â”œâ”€â”€ data/                  # Sample data for notebook experiments
â”‚   â”œâ”€â”€ EDA.ipynb              # Exploratory data analysis
â”‚   â”œâ”€â”€ data_ingestion.ipynb   # Data ingestion prototyping
â”‚   â”œâ”€â”€ data_transformation.ipynb # Data transformation experimentation
â”‚   â”œâ”€â”€ model_trainer.ipynb    # Model training and fine-tuning experiments
â”‚   â”œâ”€â”€ model_evaluation.ipynb # Model evaluation with ROUGE metrics
â”‚   â”œâ”€â”€ model_prediction.ipynb # Prediction pipeline testing
â”‚   â””â”€â”€ trail.ipynb            # Experimental trials
â”œâ”€â”€ text_summarization/        # Main package source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cloud/
â”‚   â”‚   â””â”€â”€ __init__.py        # Cloud storage operations (S3, model registry)
â”‚   â”œâ”€â”€ components/            # Core pipeline components
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py        # Downloads and extracts dataset from HuggingFace
â”‚   â”‚   â”œâ”€â”€ data_transformation.py   # Tokenizes text and prepares data for model training
â”‚   â”‚   â”œâ”€â”€ model_trainer.py         # Fine-tunes transformer model (PEGASUS/T5) for summarization
â”‚   â”‚   â””â”€â”€ model_evaluation.py      # Evaluates model using ROUGE scores
â”‚   â”œâ”€â”€ configuration/
â”‚   â”‚   â””â”€â”€ __init__.py        # Configuration manager: reads config.yaml, creates entity objects
â”‚   â”œâ”€â”€ constants/
â”‚   â”‚   â””â”€â”€ __init__.py        # Project constants: file paths, model names, environment variables
â”‚   â”œâ”€â”€ entity/
â”‚   â”‚   â””â”€â”€ __init__.py        # Dataclass entities: artifact and configuration objects
â”‚   â”œâ”€â”€ exception/
â”‚   â”‚   â””â”€â”€ __init__.py        # Custom exception handling with detailed error messages
â”‚   â”œâ”€â”€ logger/
â”‚   â”‚   â””â”€â”€ __init__.py        # Structured logging setup with timestamps
â”‚   â”œâ”€â”€ pipeline/              # Orchestration layer for training and prediction pipelines
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ stage_01_data_ingestion.py      # Orchestrates data ingestion component
â”‚   â”‚   â”œâ”€â”€ stage_02_data_transformation.py # Orchestrates data transformation component
â”‚   â”‚   â”œâ”€â”€ stage_03_model_trainer.py       # Orchestrates model training component
â”‚   â”‚   â”œâ”€â”€ stage_04_model_evaluation.py    # Orchestrates model evaluation component
â”‚   â”‚   â”œâ”€â”€ prediction_pipeline/
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py    # Prediction pipeline: loads model and generates summaries
â”‚   â”‚   â””â”€â”€ training_pipeline/
â”‚   â”‚       â””â”€â”€ __init__.py    # Training pipeline: executes all 4 stages sequentially
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ __init__.py        # Utility functions: YAML I/O, model loading, common operations
â”œâ”€â”€ .dockerignore              # Excludes unnecessary files from Docker image build
â”œâ”€â”€ .gitignore                 # Git exclusions: virtual environments, artifacts, model checkpoints
â”œâ”€â”€ Dockerfile                 # Container image for production deployment
â”œâ”€â”€ README.md                  # Project documentation and setup instructions
â”œâ”€â”€ app.py                     # FastAPI application: /predict endpoint for text summarization
â”œâ”€â”€ main.py                    # Training pipeline orchestrator: runs all 4 stages
â”œâ”€â”€ params.json                # Hyperparameters for model training (learning rate, batch size, epochs)
â”œâ”€â”€ paths.json                 # Important file paths configuration
â”œâ”€â”€ requirements.txt           # Python dependencies: transformers, datasets, evaluate, FastAPI
â””â”€â”€ setup.py                   # Package installer: configures package for pip installation
```

---

## ğŸ”§ Core Workflow

1. **Data Ingestion**  
   Uses DVC to pull training datasets (news articles, documents) from remote storage, running `stage_01_data_ingestion.py` to persist cleaned and tokenized datasets.

2. **Data Preprocessing & Tokenization**  
   Tokenizes text using SentencePiece and Hugging Face tokenizers in `stage_02_data_preprocessing.py`, preparing input sequences for transformer models.

3. **Model Training & Fine-Tuning**  
   Fine-tunes pre-trained transformer models (T5, BART, or custom architectures) through `stage_03_model_trainer.py` using PyTorch with distributed training acceleration via `accelerate`.

4. **Evaluation & Metrics**  
   Evaluates model performance using ROUGE metrics (ROUGE-1, ROUGE-2, ROUGE-L) and Hugging Face `evaluate` library in `stage_04_model_evaluation.py`, generating comprehensive performance reports.

5. **Real-Time Inference API**  
   - **FastAPI Backend** (`app.py`): Exposes `/summarize` and `/train` REST endpoints with Uvicorn ASGI server for high-performance inference.
   - **Response Times**: Sub-second summarization for documents up to 10,000 words.
   - **Deployment**: Production-ready containerized service with health checks and logging.

---

## âœ… Key Capabilities

- **State-of-the-Art NLP Models**  
  Leverages Hugging Face Transformers (v4.49.0) with support for T5, BART, PEGASUS, and custom fine-tuned models for domain-specific summarization.

- **Modular, Scalable Architecture**  
  Decoupled data ingestion, preprocessing, training, and inference layers enable easy model swapping and horizontal scaling.

- **Production-Ready MLOps Stack**  
  - **DVC** for dataset and model versioning with S3 backend integration
  - **Configuration-driven development** using YAML for reproducible experiments
  - **Structured logging** for pipeline transparency and debugging
  - **Custom exception handling** for robust error management

- **Comprehensive Evaluation**  
  Built-in ROUGE score calculation (precision, recall, F1) with support for custom evaluation metrics and automated performance tracking.

- **Distributed Training Support**  
  Utilizes Hugging Face `accelerate` library for efficient multi-GPU training and mixed-precision optimization.

- **Extensible Architecture**  
  Easily swap transformer architectures, tokenizers, or add custom preprocessing steps with minimal code changes.

---

## ğŸš€ Deployment & CI/CD

- **GitHub Actions**  
  Automated linting, testing, Docker builds, and deployment pipelines triggered on every commit (`.github/workflows/`).

- **AWS EC2 Deployment**  
  - Deployed on AWS EC2 instances with auto-scaling capabilities
  - Docker containerization for consistent environments
  - AWS ECR for secure image storage and version management
  - Environment variable management via `.env` files

- **Self-Hosted GitHub Runners**  
  Configured Linux-based self-hosted runners for cost-effective CI/CD automation with background service management.

- **Environment-Driven Configuration**  
  Manage secrets and endpoints via `.env` file:
```
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
AWS_REGION=your_region
AWS_ECR_LOGIN_URI=your_ecr_uri
ECR_REPOSITORY_NAME=your_repo_name
```

---

## ğŸƒ Running Locally

1. **Clone the repository**
```
git clone https://github.com/hasan-raza-01/Text-Summarization.git
cd Text-Summarization
```
2. **Set up environment variables**
```
cp .env.example .env
```
#### Edit .env with your AWS credentials and configuration
3. **Create virtual environment & install dependencies**
```
pip install --upgrade pip uv
uv venv
.venv\Scripts\activate # Windows
source .venv/bin/activate # Linux/Mac
uv pip install -e .
```
4. **Run the complete pipeline**
#### Execute full DVC pipeline (data ingestion â†’ training â†’ evaluation)
```
dvc repro
```
5. **Start FastAPI application**
```
uvicorn app:app --host 0.0.0.0 --port 8000 --reload
```
6. **(Alternative) Docker deployment**
```
docker build -t text-summarization:latest .
docker run -p 8000:8000 --env-file .env text-summarization:latest
```
7. **Test the API**  
Navigate to `http://localhost:8000/docs` for interactive API documentation (Swagger UI).

---

## ğŸ› ï¸ Technology Stack

- **NLP & Transformers**: Hugging Face Transformers (v4.49.0), PyTorch (v2.6.0)
- **Evaluation**: ROUGE metrics (rouge-score, rouge), Hugging Face Evaluate
- **MLOps**: DVC (v3.59.0), DVC-S3 integration
- **Backend**: FastAPI (v0.115.8), Uvicorn (v0.34.0)
- **Training Optimization**: Accelerate (v1.4.0), SentencePiece tokenization
- **Deployment**: Docker, AWS EC2, AWS ECR, GitHub Actions
- **Development**: Python-dotenv, Python-box, PyYAML, tqdm

---

## ğŸ“ Workflow Configuration

The project follows a structured workflow pattern:

1. Update `config.yaml` with dataset paths and model parameters
2. Update `.env` (optional) with AWS credentials
3. Update constants in `src/text_summarization/constants.py`
4. Update entity definitions for data structures
5. Update configuration classes
6. Update component implementations (data ingestion, training, etc.)
7. Update pipeline orchestration
8. Update `main.py` for end-to-end execution
9. Update `dvc.yaml` for pipeline stage definitions

---

## Setup GitHub Secrets

1. AWS_ACCESS_KEY_ID
2. AWS_SECRET_ACCESS_KEY
3. AWS_REGION
4. AWS_ECR_LOGIN_URI
5. ECR_REPOSITORY_NAME

---

## Docker Setup In EC2 Commands to be Executed

### Optional
```
sudo apt-get update -y
sudo apt-get upgrade -y
```
### Required
```
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker ubuntu
newgrp docker
```

---

## Add Environment Variables to EC2 Instance

### Create/edit the .env file:
```
nano /home/ubuntu/.env
```
#### Paste all variables and press CTRL+X, Y, ENTER

---

## Reconnect with GitHub Runner
```
cd actions-runner && ./run.sh
```
