{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from text_summarization.utils import read_yaml\n",
    "\n",
    "\n",
    "CONFIG = read_yaml(\"config/config.yaml\")\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConstants:\n",
    "    ARITFACTS_ROOT_DIR_NAME=CONFIG.ARITFACTS_ROOT_DIR_NAME\n",
    "    DATA_ROOT_DIR_NAME=CONFIG.DATA.ROOT_DIR_NAME\n",
    "    TRANSFORMATION_ROOT_DIR_NAME=CONFIG.DATA.TRANSFORMATION.ROOT_DIR_NAME\n",
    "    TRAIN_DATA_DIR_NAME=CONFIG.DATA.TRANSFORMATION.TRAIN_DATA_DIR_NAME\n",
    "    VALIDATION_DATA_DIR_NAME=CONFIG.DATA.TRANSFORMATION.VALIDATION_DATA_DIR_NAME\n",
    "    TEST_DATA_DIR_NAME=CONFIG.DATA.TRANSFORMATION.TEST_DATA_DIR_NAME\n",
    "    TOKENIZER_NAME=CONFIG.DATA.TRANSFORMATION.TOKENIZER_NAME\n",
    "    MODEL_REPO_ID=\"google/pegasus-cnn_dailymail\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ARITFACTS_ROOT_DIR_NAME:\", DataTransformationConstants.ARITFACTS_ROOT_DIR_NAME)\n",
    "print(\"DATA_ROOT_DIR_NAME:\", DataTransformationConstants.DATA_ROOT_DIR_NAME)\n",
    "print(\"TRANSFORMATION_ROOT_DIR_NAME:\", DataTransformationConstants.TRANSFORMATION_ROOT_DIR_NAME)\n",
    "print(\"TRAIN_DATA_DIR_NAME:\", DataTransformationConstants.TRAIN_DATA_DIR_NAME)\n",
    "print(\"VALIDATION_DATA_DIR_NAME:\", DataTransformationConstants.VALIDATION_DATA_DIR_NAME)\n",
    "print(\"TEST_DATA_DIR_NAME:\", DataTransformationConstants.TEST_DATA_DIR_NAME)\n",
    "print(\"TOKENIZER_NAME:\", DataTransformationConstants.TOKENIZER_NAME)\n",
    "print(\"MODEL_REPO_ID:\", DataTransformationConstants.MODEL_REPO_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib  import Path\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationArtifacts:\n",
    "    ARITFACTS_ROOT_DIR_PATH:Path\n",
    "    DATA_ROOT_DIR_PATH:Path\n",
    "    TRANSFORMATION_ROOT_DIR_PATH:Path\n",
    "    TRAIN_DATA_DIR_PATH:Path\n",
    "    VALIDATION_DATA_DIR_PATH:Path\n",
    "    TEST_DATA_DIR_PATH:Path\n",
    "    TOKENIZER_PATH:Path\n",
    "    MODEL_REPO_ID:str\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_summarization.configuration import __timestamp\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    ARITFACTS_ROOT_DIR_PATH=os.path.join(DataTransformationConstants.ARITFACTS_ROOT_DIR_NAME, __timestamp)\n",
    "    DATA_ROOT_DIR_PATH=os.path.join(ARITFACTS_ROOT_DIR_PATH, DataTransformationConstants.DATA_ROOT_DIR_NAME)\n",
    "    TRANSFORMATION_ROOT_DIR_PATH=os.path.join(DATA_ROOT_DIR_PATH, DataTransformationConstants.TRANSFORMATION_ROOT_DIR_NAME)\n",
    "    TRAIN_DATA_DIR_PATH=os.path.join(TRANSFORMATION_ROOT_DIR_PATH, DataTransformationConstants.TRAIN_DATA_DIR_NAME)\n",
    "    VALIDATION_DATA_DIR_PATH=os.path.join(TRANSFORMATION_ROOT_DIR_PATH, DataTransformationConstants.VALIDATION_DATA_DIR_NAME)\n",
    "    TEST_DATA_DIR_PATH=os.path.join(TRANSFORMATION_ROOT_DIR_PATH, DataTransformationConstants.TEST_DATA_DIR_NAME)\n",
    "    TOKENIZER_PATH=os.path.join(TRANSFORMATION_ROOT_DIR_PATH, DataTransformationConstants.TOKENIZER_NAME)\n",
    "    MODEL_REPO_ID=DataTransformationConstants.MODEL_REPO_ID\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ARITFACTS_ROOT_DIR_PATH:\", DataTransformationConfig.ARITFACTS_ROOT_DIR_PATH)\n",
    "print(\"DATA_ROOT_DIR_PATH:\", DataTransformationConfig.DATA_ROOT_DIR_PATH)\n",
    "print(\"TRANSFORMATION_ROOT_DIR_PATH:\", DataTransformationConfig.TRANSFORMATION_ROOT_DIR_PATH)\n",
    "print(\"TRAIN_DATA_DIR_PATH:\", DataTransformationConfig.TRAIN_DATA_DIR_PATH)\n",
    "print(\"VALIDATION_DATA_DIR_PATH:\", DataTransformationConfig.VALIDATION_DATA_DIR_PATH)\n",
    "print(\"TEST_DATA_DIR_PATH:\", DataTransformationConfig.TEST_DATA_DIR_PATH)\n",
    "print(\"TOKENIZER_PATH:\", DataTransformationConfig.TOKENIZER_PATH)\n",
    "print(\"MODEL_REPO_ID:\", DataTransformationConfig.MODEL_REPO_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from text_summarization.entity import (\n",
    "    DataIngestionArtifacts,\n",
    "    DataTransformationArtifacts\n",
    ")\n",
    "from text_summarization.exception import CustomException\n",
    "from text_summarization.logger import logging\n",
    "from text_summarization.utils import create_dirs\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "from pathlib import Path\n",
    "import torch, os, sys\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTranformationComponents:\n",
    "    __data_ingestion_config:DataIngestionArtifacts\n",
    "    __data_transformation_config:DataTransformationArtifacts\n",
    "\n",
    "    @staticmethod\n",
    "    def __get_tokenizer(repo_id:str) -> AutoTokenizer:\n",
    "        \"\"\"get tokenizer from hugging face using repo id and hugging face token\n",
    "\n",
    "        Args:\n",
    "            repo_id (str): repository id of model\n",
    "\n",
    "        Returns:\n",
    "            AutoTokenizer: tokenizer for model available in repository\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"In __get_tokenizer\")\n",
    "            # get tokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "            logging.info(f\"tokenizer pulled from {repo_id}\")\n",
    "\n",
    "            logging.info(\"Out __get_tokenizer\")\n",
    "            return tokenizer\n",
    "        except Exception as e:\n",
    "            logging.exception(e)\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "    @staticmethod\n",
    "    def __transform(record:dict, tokenizer:AutoTokenizer, device:str) -> dict:\n",
    "        \"\"\"transforms data through tokenizer\n",
    "\n",
    "        Args:\n",
    "            record (dict): input data to transform\n",
    "            tokenizer (AutoTokenizer): tokenizer to perform transformation\n",
    "\n",
    "        Returns:\n",
    "            dict: transformed data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Pre-process input text\n",
    "            tokenized_record_inputs =  tokenizer(record[\"dialogue\"], truncation=True, max_length=512).to(device)\n",
    "            \n",
    "            with tokenizer.as_target_tokenizer():\n",
    "                tokenized_record_outputs =  tokenizer(record[\"summary\"], truncation=True, max_length=128).to(device)\n",
    "\n",
    "            return {\n",
    "                \"input_ids\": tokenized_record_inputs[\"input_ids\"], \n",
    "                \"attention_mask\": tokenized_record_inputs[\"attention_mask\"], \n",
    "                \"labels\": tokenized_record_outputs[\"input_ids\"]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logging.exception(e)\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "\n",
    "    def start_data_transformation(self) -> DataTransformationArtifacts:\n",
    "        \"\"\"starts the process of data transformation\n",
    "\n",
    "        Returns:\n",
    "            DataTransformationArtifacts: path of artifacts created throughout data transformation process\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"In start_data_transformation\")\n",
    "            # create required dir's\n",
    "            create_dirs(self.__data_transformation_config.ARITFACTS_ROOT_DIR_PATH)\n",
    "            create_dirs(self.__data_transformation_config.DATA_ROOT_DIR_PATH)\n",
    "            create_dirs(self.__data_transformation_config.TRANSFORMATION_ROOT_DIR_PATH)\n",
    "            logging.info(\"Dir's creation completed\")\n",
    "\n",
    "            # get device\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            logging.info(f\"device setup to [{device}]\")\n",
    "\n",
    "            # collect required data\n",
    "            train_data = load_from_disk(os.path.join(self.__data_ingestion_config.INGESTED_ROOT_DIR_PATH, \"samsum_dataset/train\"))\n",
    "            logging.info(\"train data collected\")\n",
    "\n",
    "            validation_data = load_from_disk(os.path.join(self.__data_ingestion_config.INGESTED_ROOT_DIR_PATH, \"samsum_dataset/validation\"))\n",
    "            logging.info(\"validation data collected\")\n",
    "\n",
    "            test_data = load_from_disk(os.path.join(self.__data_ingestion_config.INGESTED_ROOT_DIR_PATH, \"samsum_dataset/test\"))\n",
    "            logging.info(\"test data collected\")\n",
    "\n",
    "            # get tokenizer\n",
    "            repo_id = self.__data_transformation_config.MODEL_REPO_ID\n",
    "            tokenizer = self.__get_tokenizer(repo_id)\n",
    "            logging.info(\"tokenizer collected successfully\")\n",
    "\n",
    "            # start transformation\n",
    "            transformed_train_data = train_data.map(self.__transform, batched=True, fn_kwargs={\"tokenizer\":tokenizer, \"device\":device})\n",
    "            transformed_validation_data = validation_data.map(self.__transform, batched=True, fn_kwargs={\"tokenizer\":tokenizer, \"device\":device})\n",
    "            transformed_test_data = test_data.map(self.__transform, batched=True, fn_kwargs={\"tokenizer\":tokenizer, \"device\":device})\n",
    "            logging.info(\"transformed data collected\")\n",
    "\n",
    "            # get variables of path for train, validation and test\n",
    "            train_data_path = self.__data_transformation_config.TRAIN_DATA_DIR_PATH\n",
    "            validation_data_path = self.__data_transformation_config.VALIDATION_DATA_DIR_PATH\n",
    "            test_data_path = self.__data_transformation_config.TEST_DATA_DIR_PATH\n",
    "            logging.info(\"created path to save transformed data\")\n",
    "\n",
    "            # save the datasets\n",
    "            transformed_train_data.save_to_disk(train_data_path)\n",
    "            logging.info(f\"transformed train data saved at {train_data_path}\")\n",
    "\n",
    "            transformed_validation_data.save_to_disk(validation_data_path)\n",
    "            logging.info(f\"transformed validation data saved at {validation_data_path}\")\n",
    "\n",
    "            transformed_test_data.save_to_disk(test_data_path)\n",
    "            logging.info(f\"transformed test data saved at {test_data_path}\")\n",
    "\n",
    "            # save tokenizer to local\n",
    "            tokenizer_path = self.__data_transformation_config.TOKENIZER_PATH\n",
    "            tokenizer.save_pretrained(Path(tokenizer_path))\n",
    "            logging.info(f\"tokenizer saved at {tokenizer_path}\")\n",
    "\n",
    "            logging.info(\"Out start_data_transformation\")\n",
    "            return self.__data_transformation_config\n",
    "        except Exception as e:\n",
    "            logging.exception(e)\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_summarization.configuration import DataIngestionConfig\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationPipeline:\n",
    "\n",
    "    def main(self) -> None:\n",
    "        self.data_ingestion = DataTranformationComponents(DataIngestionConfig, DataTransformationConfig)\n",
    "        self.data_ingestion.start_data_transformation()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "STAGE_NAME = \"Data Transformation\"\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    print(f\"\\n>>>>>>>>>>>>>>>>>>>>> {STAGE_NAME} initiated <<<<<<<<<<<<<<<<<<<<<\")\n",
    "    logging.info(f\"\\n>>>>>>>>>>>>>>>>>>>>> {STAGE_NAME} initiated <<<<<<<<<<<<<<<<<<<<<\")\n",
    "    obj = DataTransformationPipeline()\n",
    "    obj.main()\n",
    "    logging.info(f\"\\n>>>>>>>>>>>>>>>>>>>>> {STAGE_NAME} completed <<<<<<<<<<<<<<<<<<<<<\")\n",
    "    print(f\"\\n>>>>>>>>>>>>>>>>>>>>> {STAGE_NAME} completed <<<<<<<<<<<<<<<<<<<<<\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "train = load_from_disk(\"artifacts/data/transformation/train\")\n",
    "train.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = train[0]\n",
    "print(len(record[\"input_ids\"])), print(len(record[\"attention_mask\"]), print(len(record[\"labels\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"artifacts/data/transformation/tokenizer\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
